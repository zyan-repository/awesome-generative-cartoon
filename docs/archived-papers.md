# Archived Papers

Papers from 2024 and earlier that have been moved from the main README. These papers remain valuable references but are no longer featured on the front page.

> **Archive Policy:** Papers older than 1 year from the current date are automatically archived. The main README only displays papers from 2025-2026.

## Cognitive Layer (2024 and earlier)

| Paper | Venue | Links | Key Contribution |
|-------|-------|-------|------------------|
| **AgentBench** | ICLR 2024 | [![arXiv](https://img.shields.io/badge/arXiv-2308.03688-b31b1b.svg)](https://arxiv.org/abs/2308.03688) [![GitHub](https://img.shields.io/github/stars/THUDM/AgentBench.svg?style=social)](https://github.com/THUDM/AgentBench) | 8 environments, Success Rate & Pass@k metrics |
| **RolePlayBench** | NeurIPS 2024 | [![Paper](https://img.shields.io/badge/NeurIPS-2024-purple.svg)](https://proceedings.neurips.cc/paper_files/paper/2024/file/5875aca1ef70285a35940afbbce0f9fb-Paper-Datasets_and_Benchmarks_Track.pdf) | Building role-playing agents from scripts |
| **PersonaEval** | OpenReview 2024 | [![OpenReview](https://img.shields.io/badge/OpenReview-2024-green.svg)](https://openreview.net/forum?id=wZbkQStAXj) | Persona consistency benchmarking |

## Narrative Layer (2024 and earlier)

| Paper | Venue | Links | Key Contribution |
|-------|-------|-------|------------------|
| **DOME** | arXiv 2024 | [![arXiv](https://img.shields.io/badge/arXiv-2412.13575-b31b1b.svg)](https://arxiv.org/abs/2412.13575) | Dynamic hierarchical outline management |
| **StoryBench** | NeurIPS 2023 | [![Paper](https://img.shields.io/badge/NeurIPS-2023-purple.svg)](https://proceedings.neurips.cc/paper_files/paper/2023/file/f63f5fbed1a4ef08c857c5f377b5d33a-Paper-Datasets_and_Benchmarks.pdf) | Continuous story visualization benchmark |
| **Story Cloze Test** | Stanford | [![Paper](https://img.shields.io/badge/Stanford-Report-orange.svg)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1224/reports/custom_117176750.pdf) | Classic story evaluation metric |

## Visual Layer (2024 and earlier)

| Paper | Venue | Links | Key Contribution |
|-------|-------|-------|------------------|
| **DSG** | arXiv 2024 | [![arXiv](https://img.shields.io/badge/arXiv-2412.13989-b31b1b.svg)](https://arxiv.org/abs/2412.13989) | Dynamic scene graphs for evaluation |
| **CLIPScore Survey** | arXiv 2024 | [![arXiv](https://img.shields.io/badge/arXiv-2403.11821-b31b1b.svg)](https://arxiv.org/abs/2403.11821) | Quality metrics for text-to-image |
| **T2VBench** | CVPR 2024 | [![Paper](https://img.shields.io/badge/CVPR-2024-blue.svg)](https://openaccess.thecvf.com/content/CVPR2024W/EvGenFM/papers/Ji_T2VBench_Benchmarking_Temporal_Dynamics_for_Text-to-Video_Generation_CVPRW_2024_paper.pdf) | Temporal dynamics benchmark |
| **VQAScore** | GitHub | [![GitHub](https://img.shields.io/github/stars/linzhiqiu/t2v_metrics.svg?style=social)](https://github.com/linzhiqiu/t2v_metrics) | VQA-based text-image alignment |

## Audio Layer (2024 and earlier)

| Paper | Venue | Links | Key Contribution |
|-------|-------|-------|------------------|
| **LSE-D** | arXiv 2024 | [![arXiv](https://img.shields.io/badge/arXiv-2405.04327-b31b1b.svg)](https://arxiv.org/abs/2405.04327) | Lip-sync error distance metric |
| **LipFD** | NeurIPS 2024 | [![Paper](https://img.shields.io/badge/NeurIPS-2024-purple.svg)](https://proceedings.neurips.cc/paper_files/paper/2024/file/a5a5b0ff87c59172a13342d428b1e033-Paper-Conference.pdf) | Lip forgery detection |
| **Wav2Lip** | ACMMM 2020 | [![Topic](https://img.shields.io/badge/Topic-Wav2Lip-green.svg)](https://www.emergentmind.com/topics/wav2lip) | Audio-driven lip sync (foundational) |
| **SyncNet** | CVPR 2016 | - | Audio-visual sync detection (foundational) |

## Navigation

- [← Back to README](../README.md)
- [← Back to Docs](README.md)

---

# 归档论文

2024年及之前的论文已从主 README 移至此处。这些论文仍然是有价值的参考资料，但不再在首页展示。

> **归档策略：** 距今超过1年的论文会自动归档。主 README 仅显示 2025-2026 年的论文。

## 认知层（2024年及之前）

| 论文 | 会议 | 链接 | 核心贡献 |
|------|------|------|----------|
| **AgentBench** | ICLR 2024 | [![arXiv](https://img.shields.io/badge/arXiv-2308.03688-b31b1b.svg)](https://arxiv.org/abs/2308.03688) [![GitHub](https://img.shields.io/github/stars/THUDM/AgentBench.svg?style=social)](https://github.com/THUDM/AgentBench) | 8种环境，Success Rate与Pass@k指标 |
| **RolePlayBench** | NeurIPS 2024 | [![Paper](https://img.shields.io/badge/NeurIPS-2024-purple.svg)](https://proceedings.neurips.cc/paper_files/paper/2024/file/5875aca1ef70285a35940afbbce0f9fb-Paper-Datasets_and_Benchmarks_Track.pdf) | 从剧本构建角色扮演Agent |
| **PersonaEval** | OpenReview 2024 | [![OpenReview](https://img.shields.io/badge/OpenReview-2024-green.svg)](https://openreview.net/forum?id=wZbkQStAXj) | 人格一致性基准测试 |

## 叙事层（2024年及之前）

| 论文 | 会议 | 链接 | 核心贡献 |
|------|------|------|----------|
| **DOME** | arXiv 2024 | [![arXiv](https://img.shields.io/badge/arXiv-2412.13575-b31b1b.svg)](https://arxiv.org/abs/2412.13575) | 动态层次化大纲管理 |
| **StoryBench** | NeurIPS 2023 | [![Paper](https://img.shields.io/badge/NeurIPS-2023-purple.svg)](https://proceedings.neurips.cc/paper_files/paper/2023/file/f63f5fbed1a4ef08c857c5f377b5d33a-Paper-Datasets_and_Benchmarks.pdf) | 连续故事可视化基准 |
| **Story Cloze Test** | Stanford | [![Paper](https://img.shields.io/badge/Stanford-Report-orange.svg)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1224/reports/custom_117176750.pdf) | 经典故事评估指标 |

## 视觉层（2024年及之前）

| 论文 | 会议 | 链接 | 核心贡献 |
|------|------|------|----------|
| **DSG** | arXiv 2024 | [![arXiv](https://img.shields.io/badge/arXiv-2412.13989-b31b1b.svg)](https://arxiv.org/abs/2412.13989) | 动态场景图评估 |
| **CLIPScore综述** | arXiv 2024 | [![arXiv](https://img.shields.io/badge/arXiv-2403.11821-b31b1b.svg)](https://arxiv.org/abs/2403.11821) | 文本到图像质量指标 |
| **T2VBench** | CVPR 2024 | [![Paper](https://img.shields.io/badge/CVPR-2024-blue.svg)](https://openaccess.thecvf.com/content/CVPR2024W/EvGenFM/papers/Ji_T2VBench_Benchmarking_Temporal_Dynamics_for_Text-to-Video_Generation_CVPRW_2024_paper.pdf) | 时间动力学基准 |
| **VQAScore** | GitHub | [![GitHub](https://img.shields.io/github/stars/linzhiqiu/t2v_metrics.svg?style=social)](https://github.com/linzhiqiu/t2v_metrics) | 基于VQA的图文对齐 |

## 听觉层（2024年及之前）

| 论文 | 会议 | 链接 | 核心贡献 |
|------|------|------|----------|
| **LSE-D** | arXiv 2024 | [![arXiv](https://img.shields.io/badge/arXiv-2405.04327-b31b1b.svg)](https://arxiv.org/abs/2405.04327) | 口型同步误差距离指标 |
| **LipFD** | NeurIPS 2024 | [![Paper](https://img.shields.io/badge/NeurIPS-2024-purple.svg)](https://proceedings.neurips.cc/paper_files/paper/2024/file/a5a5b0ff87c59172a13342d428b1e033-Paper-Conference.pdf) | 口型伪造检测 |
| **Wav2Lip** | ACMMM 2020 | [![Topic](https://img.shields.io/badge/Topic-Wav2Lip-green.svg)](https://www.emergentmind.com/topics/wav2lip) | 音频驱动口型同步（基础性） |
| **SyncNet** | CVPR 2016 | - | 视听同步检测（基础性） |

## 导航

- [← 返回 README](../README.md)
- [← 返回文档](README.md)
